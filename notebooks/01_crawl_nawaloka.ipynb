{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 01 · Crawl Nawaloka (Depth 3) → Markdown + JSONL\n",
        "\n",
        "**Objective**: Crawl https://www.nawaloka.com to max depth 3, save as Markdown files and consolidated JSONL corpus.\n",
        "\n",
        "**Architecture**: Uses `NawalokaWebCrawler` service from `context_engineering.application.ingest_documents_service`\n",
        "\n",
        "**Provider Support**: Supports OpenRouter (multi-provider) or direct OpenAI API via `.env` configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup & Installations\n",
        "import sys\n",
        "\n",
        "# if \"google.colab\" in sys.modules or True:\n",
        "#     print(\" Installing required packages...\")\n",
        "#     %pip install -q playwright>=1.40.0 python-dotenv>=1.0.0 beautifulsoup4>=4.12.0 markdownify>=0.11.6 nest-asyncio>=1.5.0\n",
        "    \n",
        "#     # Install Playwright browsers\n",
        "#     print(\" Installing Playwright browsers...\")\n",
        "#     import subprocess\n",
        "#     subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"], check=True, capture_output=True)\n",
        "\n",
        "# print(\" Packages ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Environment loaded\n",
            " Provider: OpenRouter\n",
            " Project root: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\n"
          ]
        }
      ],
      "source": [
        "# Imports & Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "from dotenv import load_dotenv\n",
        "import nest_asyncio\n",
        "\n",
        "# Enable nested asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Load environment\n",
        "load_dotenv(project_root / \".env\")\n",
        "\n",
        "# Check for API key (OpenRouter preferred, OpenAI as fallback)\n",
        "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openrouter_key and not openai_key:\n",
        "    raise EnvironmentError(\n",
        "        \" No API key found!\\n\"\n",
        "        \" Add OPENROUTER_API_KEY (recommended) or OPENAI_API_KEY to .env\"\n",
        "    )\n",
        "\n",
        "provider = \"OpenRouter\" if openrouter_key else \"OpenAI\"\n",
        "print(\" Environment loaded\")\n",
        "print(f\" Provider: {provider}\")\n",
        "print(f\" Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CONFIGURATION (NON-SECRETS ONLY)\n",
            "============================================================\n",
            "\n",
            " Provider:\n",
            "   Provider: openrouter\n",
            "   Model Tier: general\n",
            "   Chat Model: openai/gpt-4o-mini\n",
            "   Embedding Model: openai/text-embedding-3-large\n",
            "\n",
            " Directories:\n",
            "   Data Root: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\n",
            "   Vector Store: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\vectorstore\n",
            "   Markdown: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\nawaloka_markdown\n",
            "   Cache: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\cag_cache\n",
            "\n",
            " Chunking:\n",
            "   Fixed Size: 800 tokens\n",
            "   Fixed Overlap: 100 tokens\n",
            "   Sliding Window: 512 tokens\n",
            "   Sliding Stride: 256 tokens\n",
            "   Parent-Child: 250 → 1200 tokens\n",
            "   Late Chunk: 1000 → 300 tokens\n",
            "\n",
            " Retrieval:\n",
            "   Top-K Results: 4\n",
            "   Similarity Threshold: 0.7\n",
            "\n",
            " CAG:\n",
            "   Cache TTL: 86400s\n",
            "   Max Cache Size: 1000\n",
            "\n",
            " CRAG:\n",
            "   Confidence Threshold: 0.6\n",
            "   Expanded K: 8\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            " Output directories ready:\n",
            "   - Markdown: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\nawaloka_markdown\n",
            "   - JSONL: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\n"
          ]
        }
      ],
      "source": [
        "#Load Configuration\n",
        "from context_engineering.config import (\n",
        "    validate, dump, CRAWL_OUT_DIR, MARKDOWN_DIR\n",
        ")\n",
        "\n",
        "# Validate and display config\n",
        "try:\n",
        "    validate()\n",
        "    dump()\n",
        "except Exception as e:\n",
        "    print(f\"  Config note: {e}\")\n",
        "\n",
        "# Ensure directories exist\n",
        "MARKDOWN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n Output directories ready:\")\n",
        "print(f\"   - Markdown: {MARKDOWN_DIR}\")\n",
        "print(f\"   - JSONL: {CRAWL_OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Crawler Service\n",
        "\n",
        "Using `NawalokaWebCrawler` from application layer (NOT defined here!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " NawalokaWebCrawler loaded from service layer\n",
            " Location: context_engineering.application.ingest_documents_service.web_crawler\n"
          ]
        }
      ],
      "source": [
        "# Import Web Crawler Service\n",
        "from context_engineering.application.ingest_documents_service import NawalokaWebCrawler\n",
        "\n",
        "print(\" NawalokaWebCrawler loaded from service layer\")\n",
        "print(\" Location: context_engineering.application.ingest_documents_service.web_crawler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crawl Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Crawl config:\n",
            "   - Start URLs: 15\n",
            "   - Max depth: 3\n",
            "   - Request delay: 2.0s\n"
          ]
        }
      ],
      "source": [
        "# Crawl Configuration\n",
        "BASE_URL = \"https://www.nawaloka.com\"\n",
        "\n",
        "START_PATHS = [\n",
        "    \"/\", \"/our-centres\", \"/healthchecks\", \"/channeling\",\n",
        "    \"/aboutus\", \"/contactus\", \"/blogs-and-news\", \"/emergency\",\n",
        "    \"/international\", \"/our-centres/cardiology\", \"/our-centres/oncology\",\n",
        "    \"/our-centres/neurology\", \"/our-centres/orthopedics\",\n",
        "    \"/our-centres/pediatrics\", \"/healthchecks/executive\"\n",
        "]\n",
        "\n",
        "START_URLS = [BASE_URL + path for path in START_PATHS]\n",
        "\n",
        "EXCLUDE_PATTERNS = [\n",
        "    \"/login\", \"/terms\", \"/privacy\", \"/admin\",\n",
        "    \"/images/\", \"/downloads/\", \"/media/\"\n",
        "]\n",
        "\n",
        "MAX_DEPTH = 3\n",
        "REQUEST_DELAY = 2.0\n",
        "JSONL_PATH = CRAWL_OUT_DIR / \"nawaloka_docs.jsonl\"\n",
        "\n",
        "print(f\" Crawl config:\")\n",
        "print(f\"   - Start URLs: {len(START_URLS)}\")\n",
        "print(f\"   - Max depth: {MAX_DEPTH}\")\n",
        "print(f\"   - Request delay: {REQUEST_DELAY}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute Crawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Starting crawl at 10:28:15\n",
            "\n",
            " [0] https://www.nawaloka.com/\n",
            "    Saved (36961 chars, 5 links found)\n",
            "   Progress: 1 docs saved, 1 visited, 14 in queue\n",
            " [0] https://www.nawaloka.com/our-centres\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 1 docs saved, 2 visited, 13 in queue\n",
            " [0] https://www.nawaloka.com/healthchecks\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 1 docs saved, 3 visited, 12 in queue\n",
            " [0] https://www.nawaloka.com/channeling\n",
            "    Saved (57135 chars, 5 links found)\n",
            "   Progress: 2 docs saved, 4 visited, 11 in queue\n",
            " [0] https://www.nawaloka.com/aboutus\n",
            "    Saved (37867 chars, 5 links found)\n",
            "   Progress: 3 docs saved, 5 visited, 10 in queue\n",
            " [0] https://www.nawaloka.com/contactus\n",
            "    Saved (35511 chars, 5 links found)\n",
            "   Progress: 4 docs saved, 6 visited, 9 in queue\n",
            " [0] https://www.nawaloka.com/blogs-and-news\n",
            "    Saved (36252 chars, 10 links found)\n",
            "    Added 4 new URLs to queue (depth 1)\n",
            "   Progress: 5 docs saved, 7 visited, 12 in queue\n",
            " [0] https://www.nawaloka.com/emergency\n",
            "    Saved (35888 chars, 5 links found)\n",
            "   Progress: 6 docs saved, 8 visited, 11 in queue\n",
            " [0] https://www.nawaloka.com/international\n",
            "    Saved (38785 chars, 5 links found)\n",
            "   Progress: 7 docs saved, 9 visited, 10 in queue\n",
            " [0] https://www.nawaloka.com/our-centres/cardiology\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 7 docs saved, 10 visited, 9 in queue\n",
            " [0] https://www.nawaloka.com/our-centres/oncology\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 7 docs saved, 11 visited, 8 in queue\n",
            " [0] https://www.nawaloka.com/our-centres/neurology\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 7 docs saved, 12 visited, 7 in queue\n",
            " [0] https://www.nawaloka.com/our-centres/orthopedics\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 7 docs saved, 13 visited, 6 in queue\n",
            " [0] https://www.nawaloka.com/our-centres/pediatrics\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 7 docs saved, 14 visited, 5 in queue\n",
            " [0] https://www.nawaloka.com/healthchecks/executive\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 7 docs saved, 15 visited, 4 in queue\n",
            " [1] https://www.nawaloka.com/blogs-and-news/laparoscopic-surgery-cost-in-sri-lanka\n",
            "    Saved (43178 chars, 7 links found)\n",
            "    Added 1 new URLs to queue (depth 2)\n",
            "   Progress: 8 docs saved, 16 visited, 4 in queue\n",
            " [1] https://www.nawaloka.com/blog/laparoscopic-surgery-cost-in-sri-lanka\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 8 docs saved, 17 visited, 3 in queue\n",
            " [1] https://www.nawaloka.com/blog/best-cardiologist-in-sri-lanka\n",
            "     Skipped (content too short: 52 chars)\n",
            "   Progress: 8 docs saved, 18 visited, 2 in queue\n",
            " [1] https://www.nawaloka.com/blogs-and-news/best-cardiologist-in-sri-lanka\n",
            "    Saved (45216 chars, 7 links found)\n",
            "   Progress: 9 docs saved, 19 visited, 1 in queue\n",
            " [2] https://www.nawaloka.com\n",
            "    Saved (36961 chars, 6 links found)\n",
            "   Progress: 10 docs saved, 20 visited, 0 in queue\n",
            "\n",
            " Crawl complete in 243.3s\n",
            " Documents collected: 10\n",
            " URLs visited: 20\n"
          ]
        }
      ],
      "source": [
        "# Run Crawl with Service\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize crawler service\n",
        "crawler = NawalokaWebCrawler(\n",
        "    base_url=BASE_URL,\n",
        "    max_depth=MAX_DEPTH,\n",
        "    exclude_patterns=EXCLUDE_PATTERNS\n",
        ")\n",
        "\n",
        "def run_crawler_thread():\n",
        "    # 1. Set Proactor policy (required for Playwright on Windows)\n",
        "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
        "    \n",
        "    # 2. Create a new event loop for this thread\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    \n",
        "    try:\n",
        "        # 3. Process the async crawl directly on this loop\n",
        "        return loop.run_until_complete(crawler.crawl_async(START_URLS, request_delay=REQUEST_DELAY))\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "print(f\"\\n Starting crawl at {time.strftime('%H:%M:%S')}\\n\")\n",
        "\n",
        "# Run in a separate thread to avoid conflict with Jupyter's running loop\n",
        "with ThreadPoolExecutor(max_workers=1) as executor:\n",
        "    future = executor.submit(run_crawler_thread)\n",
        "    documents = future.result()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n Crawl complete in {elapsed:.1f}s\")\n",
        "print(f\" Documents collected: {len(documents)}\")\n",
        "print(f\" URLs visited: {len(crawler.visited)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Saved 10 markdown files to d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\nawaloka_markdown\n",
            " Saved JSONL corpus to d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\nawaloka_docs.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Save Outputs\n",
        "# Save markdown files\n",
        "for i, doc in enumerate(documents):\n",
        "    url_path = urlparse(doc['url']).path.strip('/').replace('/', '_')\n",
        "    if not url_path:\n",
        "        url_path = \"homepage\"\n",
        "    filename = f\"{i:03d}_{url_path}.md\"\n",
        "    \n",
        "    md_file = MARKDOWN_DIR / filename\n",
        "    with open(md_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# {doc['title']}\\n\\n\")\n",
        "        f.write(f\"**URL**: {doc['url']}\\n\\n\")\n",
        "        f.write(f\"**Depth**: {doc['depth_level']}\\n\\n\")\n",
        "        f.write(\"---\\n\\n\")\n",
        "        f.write(doc['content'])\n",
        "\n",
        "print(f\" Saved {len(documents)} markdown files to {MARKDOWN_DIR}\")\n",
        "\n",
        "# Save JSONL\n",
        "with open(JSONL_PATH, 'w', encoding='utf-8') as f:\n",
        "    for doc in documents:\n",
        "        f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\" Saved JSONL corpus to {JSONL_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Quality Checks:\n",
            "\n",
            "1️  Markdown files: 10\n",
            "     Only 10 pages (site may be small)\n",
            "\n",
            "2️  JSONL file: 411,376 bytes\n",
            "\n",
            "3️  Random samples:\n",
            "\n",
            "   Sample 1:\n",
            "   - URL: https://www.nawaloka.com/blogs-and-news/best-cardiologist-in-sri-lanka\n",
            "   - Title: Finding the Best Cardiologist in Sri Lanka | Nawaloka Hospitals\n",
            "   - Words: 1594\n",
            "\n",
            "   Sample 2:\n",
            "   - URL: https://www.nawaloka.com/aboutus\n",
            "   - Title: About Nawaloka Hospital | Best colombo private hospitals\n",
            "   - Words: 430\n",
            "\n",
            "   Sample 3:\n",
            "   - URL: https://www.nawaloka.com/channeling\n",
            "   - Title: Doctor Channeling | Nawaloka Hospitals\n",
            "   - Words: 1759\n",
            "\n",
            " All quality checks passed!\n"
          ]
        }
      ],
      "source": [
        "# Quality Checks\n",
        "import random\n",
        "\n",
        "print(\" Quality Checks:\\n\")\n",
        "\n",
        "# Check markdown files\n",
        "md_files = list(MARKDOWN_DIR.glob(\"*.md\"))\n",
        "print(f\"1️  Markdown files: {len(md_files)}\")\n",
        "\n",
        "if len(md_files) >= 20:\n",
        "    print(f\"    Good! Got {len(md_files)} pages\")\n",
        "elif len(md_files) >= 10:\n",
        "    print(f\"     Only {len(md_files)} pages (site may be small)\")\n",
        "else:\n",
        "    raise AssertionError(f\" Too few pages: {len(md_files)}\")\n",
        "\n",
        "# Check JSONL\n",
        "assert JSONL_PATH.exists(), f\" JSONL not found\"\n",
        "print(f\"\\n2️  JSONL file: {JSONL_PATH.stat().st_size:,} bytes\")\n",
        "\n",
        "# Sample inspection\n",
        "with open(JSONL_PATH, 'r', encoding='utf-8') as f:\n",
        "    all_docs = [json.loads(line) for line in f]\n",
        "\n",
        "samples = random.sample(all_docs, min(3, len(all_docs)))\n",
        "print(f\"\\n3️  Random samples:\\n\")\n",
        "\n",
        "for i, doc in enumerate(samples, 1):\n",
        "    print(f\"   Sample {i}:\")\n",
        "    print(f\"   - URL: {doc['url']}\")\n",
        "    print(f\"   - Title: {doc['title']}\")\n",
        "    print(f\"   - Words: {len(doc['content'].split())}\")\n",
        "    print()\n",
        "\n",
        "print(\" All quality checks passed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sahas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
