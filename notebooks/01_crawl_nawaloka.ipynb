{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 01 · Crawl Nawaloka (Depth 3) → Markdown + JSONL\n",
        "\n",
        "**Objective**: Crawl https://www.nawaloka.com to max depth 3, save as Markdown files and consolidated JSONL corpus.\n",
        "\n",
        "**Architecture**: Uses `NawalokaWebCrawler` service from `context_engineering.application.ingest_documents_service`\n",
        "\n",
        "**Provider Support**: Supports OpenRouter (multi-provider) or direct OpenAI API via `.env` configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup & Installations\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules or True:\n",
        "    print(\" Installing required packages...\")\n",
        "    %pip install -q playwright>=1.40.0 python-dotenv>=1.0.0 beautifulsoup4>=4.12.0 markdownify>=0.11.6 nest-asyncio>=1.5.0\n",
        "    \n",
        "    # Install Playwright browsers\n",
        "    print(\" Installing Playwright browsers...\")\n",
        "    import subprocess\n",
        "    subprocess.run([sys.executable, \"-m\", \"playwright\", \"install\", \"chromium\"], check=True, capture_output=True)\n",
        "\n",
        "print(\" Packages ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Imports & Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "from dotenv import load_dotenv\n",
        "import nest_asyncio\n",
        "\n",
        "# Enable nested asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Load environment\n",
        "load_dotenv(project_root / \".env\")\n",
        "\n",
        "# Check for API key (OpenRouter preferred, OpenAI as fallback)\n",
        "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openrouter_key and not openai_key:\n",
        "    raise EnvironmentError(\n",
        "        \" No API key found!\\n\"\n",
        "        \" Add OPENROUTER_API_KEY (recommended) or OPENAI_API_KEY to .env\"\n",
        "    )\n",
        "\n",
        "provider = \"OpenRouter\" if openrouter_key else \"OpenAI\"\n",
        "print(\" Environment loaded\")\n",
        "print(f\" Provider: {provider}\")\n",
        "print(f\" Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Load Configuration\n",
        "from context_engineering.config import (\n",
        "    validate, dump, CRAWL_OUT_DIR, MARKDOWN_DIR\n",
        ")\n",
        "\n",
        "# Validate and display config\n",
        "try:\n",
        "    validate()\n",
        "    dump()\n",
        "except Exception as e:\n",
        "    print(f\"  Config note: {e}\")\n",
        "\n",
        "# Ensure directories exist\n",
        "MARKDOWN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n Output directories ready:\")\n",
        "print(f\"   - Markdown: {MARKDOWN_DIR}\")\n",
        "print(f\"   - JSONL: {CRAWL_OUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Crawler Service\n",
        "\n",
        "Using `NawalokaWebCrawler` from application layer (NOT defined here!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Import Web Crawler Service\n",
        "from context_engineering.application.ingest_documents_service import NawalokaWebCrawler\n",
        "\n",
        "print(\" NawalokaWebCrawler loaded from service layer\")\n",
        "print(\" Location: context_engineering.application.ingest_documents_service.web_crawler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crawl Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Crawl Configuration\n",
        "BASE_URL = \"https://www.nawaloka.com\"\n",
        "\n",
        "START_PATHS = [\n",
        "    \"/\", \"/our-centres\", \"/healthchecks\", \"/channeling\",\n",
        "    \"/aboutus\", \"/contactus\", \"/blogs-and-news\", \"/emergency\",\n",
        "    \"/international\", \"/our-centres/cardiology\", \"/our-centres/oncology\",\n",
        "    \"/our-centres/neurology\", \"/our-centres/orthopedics\",\n",
        "    \"/our-centres/pediatrics\", \"/healthchecks/executive\"\n",
        "]\n",
        "\n",
        "START_URLS = [BASE_URL + path for path in START_PATHS]\n",
        "\n",
        "EXCLUDE_PATTERNS = [\n",
        "    \"/login\", \"/terms\", \"/privacy\", \"/admin\",\n",
        "    \"/images/\", \"/downloads/\", \"/media/\"\n",
        "]\n",
        "\n",
        "MAX_DEPTH = 3\n",
        "REQUEST_DELAY = 2.0\n",
        "JSONL_PATH = CRAWL_OUT_DIR / \"nawaloka_docs.jsonl\"\n",
        "\n",
        "print(f\" Crawl config:\")\n",
        "print(f\"   - Start URLs: {len(START_URLS)}\")\n",
        "print(f\"   - Max depth: {MAX_DEPTH}\")\n",
        "print(f\"   - Request delay: {REQUEST_DELAY}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute Crawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Run Crawl with Service\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize crawler service\n",
        "crawler = NawalokaWebCrawler(\n",
        "    base_url=BASE_URL,\n",
        "    max_depth=MAX_DEPTH,\n",
        "    exclude_patterns=EXCLUDE_PATTERNS\n",
        ")\n",
        "\n",
        "print(f\"\\n Starting crawl at {time.strftime('%H:%M:%S')}\\n\")\n",
        "documents = crawler.crawl(START_URLS, request_delay=REQUEST_DELAY)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n Crawl complete in {elapsed:.1f}s\")\n",
        "print(f\" Documents collected: {len(documents)}\")\n",
        "print(f\" URLs visited: {len(crawler.visited)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Save Outputs\n",
        "# Save markdown files\n",
        "for i, doc in enumerate(documents):\n",
        "    url_path = urlparse(doc['url']).path.strip('/').replace('/', '_')\n",
        "    if not url_path:\n",
        "        url_path = \"homepage\"\n",
        "    filename = f\"{i:03d}_{url_path}.md\"\n",
        "    \n",
        "    md_file = MARKDOWN_DIR / filename\n",
        "    with open(md_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# {doc['title']}\\n\\n\")\n",
        "        f.write(f\"**URL**: {doc['url']}\\n\\n\")\n",
        "        f.write(f\"**Depth**: {doc['depth_level']}\\n\\n\")\n",
        "        f.write(\"---\\n\\n\")\n",
        "        f.write(doc['content'])\n",
        "\n",
        "print(f\" Saved {len(documents)} markdown files to {MARKDOWN_DIR}\")\n",
        "\n",
        "# Save JSONL\n",
        "with open(JSONL_PATH, 'w', encoding='utf-8') as f:\n",
        "    for doc in documents:\n",
        "        f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\" Saved JSONL corpus to {JSONL_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Quality Checks\n",
        "import random\n",
        "\n",
        "print(\" Quality Checks:\\n\")\n",
        "\n",
        "# Check markdown files\n",
        "md_files = list(MARKDOWN_DIR.glob(\"*.md\"))\n",
        "print(f\"1️  Markdown files: {len(md_files)}\")\n",
        "\n",
        "if len(md_files) >= 20:\n",
        "    print(f\"    Good! Got {len(md_files)} pages\")\n",
        "elif len(md_files) >= 10:\n",
        "    print(f\"     Only {len(md_files)} pages (site may be small)\")\n",
        "else:\n",
        "    raise AssertionError(f\" Too few pages: {len(md_files)}\")\n",
        "\n",
        "# Check JSONL\n",
        "assert JSONL_PATH.exists(), f\" JSONL not found\"\n",
        "print(f\"\\n2️  JSONL file: {JSONL_PATH.stat().st_size:,} bytes\")\n",
        "\n",
        "# Sample inspection\n",
        "with open(JSONL_PATH, 'r', encoding='utf-8') as f:\n",
        "    all_docs = [json.loads(line) for line in f]\n",
        "\n",
        "samples = random.sample(all_docs, min(3, len(all_docs)))\n",
        "print(f\"\\n3️  Random samples:\\n\")\n",
        "\n",
        "for i, doc in enumerate(samples, 1):\n",
        "    print(f\"   Sample {i}:\")\n",
        "    print(f\"   - URL: {doc['url']}\")\n",
        "    print(f\"   - Title: {doc['title']}\")\n",
        "    print(f\"   - Words: {len(doc['content'].split())}\")\n",
        "    print()\n",
        "\n",
        "print(\" All quality checks passed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sahas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
