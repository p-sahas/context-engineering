{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 01 · Chunk (3 Strategies) → Build Vector Index\n",
        "\n",
        "**Objective**: Transform crawled docs into retrieval chunks using three strategies, embed with OpenRouter/OpenAI, build ChromaDB persistent index.\n",
        "\n",
        "**Architecture**: Uses chunking functions from `context_engineering.application.ingest_documents_service.chunkers`\n",
        "\n",
        "**Provider Support**: Uses OpenRouter unified API (access OpenAI, Anthropic, Google, etc. with one key) or direct OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Packages ready\n"
          ]
        }
      ],
      "source": [
        "#  Setup & Installations\n",
        "import sys\n",
        "\n",
        "# if \"google.colab\" in sys.modules or True:\n",
        "#     print(\" Installing required packages...\")\n",
        "#     %pip install -q langchain>=0.1.0 langchain-openai>=0.0.5 langchain-community>=0.0.20 langchain-text-splitters>=0.2.0 chromadb>=0.4.0 tiktoken>=0.5.0 python-dotenv>=1.0.0\n",
        "\n",
        "print(\" Packages ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            " Environment loaded\n",
            " Provider: OpenRouter\n",
            " Project root: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\n"
          ]
        }
      ],
      "source": [
        "#  Imports & Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Load environment\n",
        "load_dotenv(project_root / \".env\")\n",
        "\n",
        "# Check for API key (OpenRouter preferred, OpenAI as fallback)\n",
        "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openrouter_key and not openai_key:\n",
        "    raise EnvironmentError(\n",
        "        \"   No API key found!\\n\"\n",
        "        \"   Add OPENROUTER_API_KEY (recommended) or OPENAI_API_KEY to .env\"\n",
        "    )\n",
        "\n",
        "# Load configuration\n",
        "from context_engineering.config import (\n",
        "    CRAWL_OUT_DIR, VECTOR_DIR, EMBEDDING_MODEL, PROVIDER\n",
        ")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "provider = \"OpenRouter\" if openrouter_key else \"OpenAI\"\n",
        "print(\" Environment loaded\")\n",
        "print(f\" Provider: {provider}\")\n",
        "print(f\" Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Chunking Services\n",
        "\n",
        "Using chunking functions from application layer (NOT defined here!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Chunking services loaded from service layer\n",
            " Location: context_engineering.application.ingest_documents_service.chunkers\n",
            "\n",
            " Available strategies:\n",
            "   1. semantic_chunk  - Split by heading structure\n",
            "   2. fixed_chunk     - Uniform 800-token chunks with overlap\n",
            "   3. sliding_chunk   - Overlapping windows for better recall\n"
          ]
        }
      ],
      "source": [
        "#  Import Chunking Services\n",
        "from context_engineering.application.ingest_documents_service import (\n",
        "    semantic_chunk,\n",
        "    fixed_chunk,\n",
        "    sliding_chunk\n",
        ")\n",
        "\n",
        "print(\" Chunking services loaded from service layer\")\n",
        "print(\" Location: context_engineering.application.ingest_documents_service.chunkers\")\n",
        "print(\"\\n Available strategies:\")\n",
        "print(\"   1. semantic_chunk  - Split by heading structure\")\n",
        "print(\"   2. fixed_chunk     - Uniform 800-token chunks with overlap\")\n",
        "print(\"   3. sliding_chunk   - Overlapping windows for better recall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loaded 10 documents\n",
            " Total content size: 403,754 chars\n"
          ]
        }
      ],
      "source": [
        "#  Load Corpus\n",
        "jsonl_path = CRAWL_OUT_DIR / \"nawaloka_docs.jsonl\"\n",
        "\n",
        "if not jsonl_path.exists():\n",
        "    raise FileNotFoundError(f\" Corpus not found. Run 01_crawl_nawaloka.ipynb first.\")\n",
        "\n",
        "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "    documents = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\" Loaded {len(documents)} documents\")\n",
        "print(f\" Total content size: {sum(len(d['content']) for d in documents):,} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Chunking Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Attempting to clean: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\vectorstore\n",
            "    Cleaned up successfully\n",
            " Fresh vector directory ready: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\vectorstore\n"
          ]
        }
      ],
      "source": [
        "# Cleanup Vector Store (prevents corruption)\n",
        "import shutil\n",
        "import os\n",
        "import stat\n",
        "import time\n",
        "\n",
        "def on_rm_error(func, path, exc_info):\n",
        "    # Error handler for shutil.rmtree\n",
        "    try:\n",
        "        os.chmod(path, stat.S_IWRITE)\n",
        "        func(path)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Try to remove existing vector store\n",
        "LOCK_DETECTED = False\n",
        "if VECTOR_DIR.exists():\n",
        "    print(f\" Attempting to clean: {VECTOR_DIR}\")\n",
        "    try:\n",
        "        shutil.rmtree(VECTOR_DIR, onerror=on_rm_error)\n",
        "        print(\"    Cleaned up successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Cleanup failed ({e})\")\n",
        "        # Try renaming as last resort cleanup\n",
        "        try:\n",
        "             backup = VECTOR_DIR.with_name(f\"vectorstore_locked_{int(time.time())}\")\n",
        "             os.rename(VECTOR_DIR, backup)\n",
        "             print(f\"    Renamed locked dir to: {backup}\")\n",
        "        except Exception as e2:\n",
        "             LOCK_DETECTED = True\n",
        "             print(f\"    CRITICAL LOCK: Could not delete or rename ({e2})\")\n",
        "\n",
        "if LOCK_DETECTED:\n",
        "    # OVERRIDE VECTOR_DIR to use a fresh path\n",
        "    print(\"\\n ⚠️  FILE LOCK DETECTED (likely opened in editor)\")\n",
        "    print(\"    Switching to a new directory to bypass lock...\")\n",
        "    VECTOR_DIR = VECTOR_DIR.with_name(\"vectorstore_v2\")\n",
        "    print(f\"    NEW TARGET: {VECTOR_DIR}\")\n",
        "else:\n",
        "    # Create fresh standard directory\n",
        "    VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\" Fresh vector directory ready: {VECTOR_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Running semantic chunking...\n",
            " Semantic chunking complete: 238 chunks\n",
            " Saved to: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\chunks_semantic.jsonl\n"
          ]
        }
      ],
      "source": [
        "#  Semantic Chunking (using service!)\n",
        "print(\" Running semantic chunking...\")\n",
        "semantic_chunks = semantic_chunk(documents)\n",
        "\n",
        "# Save\n",
        "semantic_path = CRAWL_OUT_DIR / \"chunks_semantic.jsonl\"\n",
        "with open(semantic_path, 'w', encoding='utf-8') as f:\n",
        "    for chunk in semantic_chunks:\n",
        "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\" Semantic chunking complete: {len(semantic_chunks)} chunks\")\n",
        "print(f\" Saved to: {semantic_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Running fixed-window chunking...\n",
            " Fixed chunking complete: 237 chunks\n",
            " Avg token count: 1186.7\n",
            " Saved to: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\chunks_fixed.jsonl\n"
          ]
        }
      ],
      "source": [
        "#  Fixed-Window Chunking (using service!)\n",
        "print(\" Running fixed-window chunking...\")\n",
        "fixed_chunks = fixed_chunk(documents)\n",
        "\n",
        "# Save\n",
        "fixed_path = CRAWL_OUT_DIR / \"chunks_fixed.jsonl\"\n",
        "with open(fixed_path, 'w', encoding='utf-8') as f:\n",
        "    for chunk in fixed_chunks:\n",
        "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
        "\n",
        "avg_tokens = sum(c['token_count'] for c in fixed_chunks) / len(fixed_chunks) if fixed_chunks else 0\n",
        "print(f\" Fixed chunking complete: {len(fixed_chunks)} chunks\")\n",
        "print(f\" Avg token count: {avg_tokens:.1f}\")\n",
        "print(f\" Saved to: {fixed_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Running sliding-window chunking...\n",
            " Sliding chunking complete: 400 chunks\n",
            " Saved to: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\chunks_sliding.jsonl\n"
          ]
        }
      ],
      "source": [
        "#  Sliding-Window Chunking (using service!)\n",
        "print(\" Running sliding-window chunking...\")\n",
        "sliding_chunks = sliding_chunk(documents)\n",
        "\n",
        "# Save\n",
        "sliding_path = CRAWL_OUT_DIR / \"chunks_sliding.jsonl\"\n",
        "with open(sliding_path, 'w', encoding='utf-8') as f:\n",
        "    for chunk in sliding_chunks:\n",
        "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\" Sliding chunking complete: {len(sliding_chunks)} chunks\")\n",
        "print(f\" Saved to: {sliding_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spot-Check Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Spot-Check: 2 samples from each strategy\n",
            "\n",
            "============================================================\n",
            "SEMANTIC SAMPLES\n",
            "============================================================\n",
            "**Semantic** chunk:\n",
            "  URL: https://www.nawaloka.com/blogs-and-news/laparoscopic-surgery-cost-in-sri-lanka\n",
            "  Strategy: semantic\n",
            "  Text length: 4000 chars\n",
            "  Preview: UpShWJWoanYLK1CsLNgavS7dHMlDsrpR1F5tPdHcbe/wBR2rb6UbqEjGytLHbFa4vGotlbjy7VCckJtMQR7fAQUpQlXbq3PqRtuT...\n",
            "\n",
            "**Semantic** chunk:\n",
            "  URL: https://www.nawaloka.com/channeling\n",
            "  Strategy: semantic\n",
            "  Text length: 3999 chars\n",
            "  Preview: ![Physiotherapy](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAA...\n",
            "\n",
            "============================================================\n",
            "FIXED-WINDOW SAMPLES\n",
            "============================================================\n",
            "**Fixed** chunk:\n",
            "  URL: https://www.nawaloka.com/\n",
            "  Strategy: fixed\n",
            "  Text length: 31 chars\n",
            "  Preview: Doctor BookingsFind your Doctor...\n",
            "\n",
            "**Fixed** chunk:\n",
            "  URL: https://www.nawaloka.com/blogs-and-news/laparoscopic-surgery-cost-in-sri-lanka\n",
            "  Strategy: fixed\n",
            "  Text length: 2466 chars\n",
            "  Preview: #### After Surgery\n",
            "\n",
            "* Patients typically stay for a shorter duration, sometimes just overnight.\n",
            "* Re...\n",
            "\n",
            "============================================================\n",
            "SLIDING-WINDOW SAMPLES\n",
            "============================================================\n",
            "**Sliding** chunk:\n",
            "  URL: https://www.nawaloka.com/contactus\n",
            "  Strategy: sliding\n",
            "  Text length: 2048 chars\n",
            "  Preview: N05yVUBStyIVwBca+gcHxAfUKqBso8LGuWKLWXcJk3JlPZ61qEoK+iEfvB+aRTIho59Wm3rvWGa3pFhAyTR9Y0cbZvMa/NXJjana...\n",
            "\n",
            "**Sliding** chunk:\n",
            "  URL: https://www.nawaloka.com/aboutus\n",
            "  Strategy: sliding\n",
            "  Text length: 2048 chars\n",
            "  Preview: G9onXD4xNV+w97WHaJ1w+MTVfsPe1jXEy98Tfw/RTclU8nyfIza3vyovwvi06l97DdsfnXtX8M3+jh2G7Y/Ovav4Zv8ARx87ROuH...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  Spot-Check Samples\n",
        "print(\" Spot-Check: 2 samples from each strategy\\n\")\n",
        "\n",
        "def print_sample(chunk, strategy_name):\n",
        "    print(f\"**{strategy_name}** chunk:\")\n",
        "    print(f\"  URL: {chunk['url']}\")\n",
        "    print(f\"  Strategy: {chunk['strategy']}\")\n",
        "    print(f\"  Text length: {len(chunk['text'])} chars\")\n",
        "    print(f\"  Preview: {chunk['text'][:100]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SEMANTIC SAMPLES\")\n",
        "print(\"=\" * 60)\n",
        "for chunk in random.sample(semantic_chunks, min(2, len(semantic_chunks))):\n",
        "    print_sample(chunk, \"Semantic\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FIXED-WINDOW SAMPLES\")\n",
        "print(\"=\" * 60)\n",
        "for chunk in random.sample(fixed_chunks, min(2, len(fixed_chunks))):\n",
        "    print_sample(chunk, \"Fixed\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SLIDING-WINDOW SAMPLES\")\n",
        "print(\"=\" * 60)\n",
        "for chunk in random.sample(sliding_chunks, min(2, len(sliding_chunks))):\n",
        "    print_sample(chunk, \"Sliding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build ChromaDB Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Embeddings initialized: openai/text-embedding-3-large\n",
            " Provider: openrouter\n",
            " Total chunks to embed: 875\n",
            "\n",
            " Creating Chroma vector store...\n",
            "\n",
            " Vector store created!\n",
            " Total vectors indexed: 875\n"
          ]
        }
      ],
      "source": [
        "#  Build Vector Index with LangChain\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from context_engineering.infrastructure.llm_providers import get_default_embeddings\n",
        "\n",
        "# Initialize embeddings using service factory (supports OpenRouter)\n",
        "embeddings = get_default_embeddings()\n",
        "\n",
        "print(f\" Embeddings initialized: {EMBEDDING_MODEL}\")\n",
        "print(f\" Provider: {PROVIDER}\")\n",
        "\n",
        "# Prepare directory\n",
        "VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Combine all chunks\n",
        "all_chunks = semantic_chunks + fixed_chunks + sliding_chunks\n",
        "print(f\" Total chunks to embed: {len(all_chunks)}\")\n",
        "\n",
        "# Convert to LangChain Documents\n",
        "lc_documents = []\n",
        "for chunk in all_chunks:\n",
        "    doc = Document(\n",
        "        page_content=chunk['text'],\n",
        "        metadata={\n",
        "            \"url\": chunk['url'],\n",
        "            \"title\": chunk['title'],\n",
        "            \"strategy\": chunk['strategy'],\n",
        "            \"chunk_index\": chunk['chunk_index']\n",
        "        }\n",
        "    )\n",
        "    lc_documents.append(doc)\n",
        "\n",
        "print(f\"\\n Creating Chroma vector store...\\n\")\n",
        "\n",
        "# Create vector store (LangChain handles batching + retries)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=lc_documents,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=str(VECTOR_DIR),\n",
        "    collection_name=\"nawaloka\"\n",
        ")\n",
        "\n",
        "print(f\" Vector store created!\")\n",
        "print(f\" Total vectors indexed: {vectorstore._collection.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Index Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Index Sanity Check\n",
            "\n",
            " Collection contains 875 vectors\n",
            "\n",
            " Test query: 'cardiology services and health checks'\n",
            "\n",
            "Top 3 results:\n",
            "\n",
            "1. Score: 1.0076\n",
            "   URL: https://www.nawaloka.com/blogs-and-news/best-cardiologist-in-sri-lanka\n",
            "   Strategy: sliding\n",
            "   Preview: IIkIQgiQhCCJCEIIkIQgiQhCCJCEIIkIQgi//9k=)\n",
            "\n",
            "Service BookingsBook your service here\n",
            "\n",
            "# Finding the Bes...\n",
            "\n",
            "2. Score: 1.0355\n",
            "   URL: https://www.nawaloka.com/blogs-and-news/best-cardiologist-in-sri-lanka\n",
            "   Strategy: sliding\n",
            "   Preview: le to yours.\n",
            "6. **Read Patient Reviews** - Web based reviews are a source of valuable information ab...\n",
            "\n",
            "3. Score: 1.0382\n",
            "   URL: https://www.nawaloka.com/blogs-and-news/best-cardiologist-in-sri-lanka\n",
            "   Strategy: sliding\n",
            "   Preview: xsDLOyP88dN/ap/70qqc9Ff+5V7WHvSqpz0V/wC5V7WIB2GbNXP1X/qX7GHYZs1c/Vf+pfsYzeT7R/AosND5Bn/Y+qn/AL0qqc9F...\n",
            "\n",
            " Index sanity check passed!\n",
            " Vector store persisted at: d:\\Courses\\_Zuu Crew\\AI Engineer Essentials\\Programming\\Context Engineering\\data\\vectorstore\n"
          ]
        }
      ],
      "source": [
        "#  Index Sanity Check\n",
        "print(\" Index Sanity Check\\n\")\n",
        "\n",
        "# Verify count\n",
        "count = vectorstore._collection.count()\n",
        "print(f\" Collection contains {count} vectors\")\n",
        "assert count > 0, \" Collection is empty!\"\n",
        "\n",
        "# Test query\n",
        "test_query = \"cardiology services and health checks\"\n",
        "print(f\"\\n Test query: '{test_query}'\\n\")\n",
        "\n",
        "results = vectorstore.similarity_search_with_score(\n",
        "    query=test_query,\n",
        "    k=3\n",
        ")\n",
        "\n",
        "print(\"Top 3 results:\")\n",
        "for i, (doc, score) in enumerate(results, 1):\n",
        "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
        "    print(f\"   URL: {doc.metadata['url']}\")\n",
        "    print(f\"   Strategy: {doc.metadata['strategy']}\")\n",
        "    print(f\"   Preview: {doc.page_content[:100]}...\")\n",
        "\n",
        "print(\"\\n Index sanity check passed!\")\n",
        "print(f\" Vector store persisted at: {VECTOR_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sahas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
