{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 01 · Advanced RAG with Services\n",
        "\n",
        "**Objective**: Build RAG, CAG, and CRAG using service layer.\n",
        "\n",
        "**Architecture**: Uses `RAGService`, `CAGService`, `CRAGService` from `context_engineering.application.chat_service`\n",
        "\n",
        "**Provider Support**: Uses OpenRouter unified API for multi-provider LLM access (GPT-4o, Claude, Gemini, etc.) or direct OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Setup & Installations\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules or True:\n",
        "    print(\" Installing required packages...\")\n",
        "    %pip install -q langchain-core>=0.1.0 langchain-openai>=0.0.5 langchain-community>=0.0.20 chromadb>=0.4.0 python-dotenv>=1.0.0\n",
        "\n",
        "print(\" Packages ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Imports & Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "# Load environment\n",
        "load_dotenv(project_root / \".env\")\n",
        "\n",
        "# Check for API key (OpenRouter preferred, OpenAI as fallback)\n",
        "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not openrouter_key and not openai_key:\n",
        "    raise EnvironmentError(\n",
        "        \"   No API key found!\\n\"\n",
        "        \"   Add OPENROUTER_API_KEY (recommended) or OPENAI_API_KEY to .env\"\n",
        "    )\n",
        "\n",
        "# Load configuration\n",
        "from context_engineering.config import (\n",
        "    VECTOR_DIR, CACHE_DIR, TOP_K_RESULTS,\n",
        "    CHAT_MODEL, EMBEDDING_MODEL, PROVIDER\n",
        ")\n",
        "\n",
        "provider = \"OpenRouter\" if openrouter_key else \"OpenAI\"\n",
        "print(\" Environment loaded\")\n",
        "print(f\" Provider: {provider}\")\n",
        "print(f\" Project root: {project_root}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Chat Services\n",
        "\n",
        "Using RAG/CAG/CRAG services from application layer (NOT defined here!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Import Chat Services\n",
        "from context_engineering.application.chat_service import (\n",
        "    RAGService,\n",
        "    CAGService,\n",
        "    CRAGService,\n",
        "    CAGCache\n",
        ")\n",
        "\n",
        "print(\" Chat services loaded from service layer\")\n",
        "print(\" Location: context_engineering.application.chat_service\")\n",
        "print(\"\\n Available services:\")\n",
        "print(\"   1. RAGService   - Standard RAG with modern LCEL\")\n",
        "print(\"   2. CAGService   - Cache-Augmented Generation (semantic)\")\n",
        "print(\"   3. CRAGService  - Corrective RAG with confidence scoring\")\n",
        "print(\"   4. CAGCache     - Semantic cache (FAQs + History)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Connect to Vector Store & Initialize LLM\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from context_engineering.infrastructure.llm_providers import (\n",
        "    get_default_embeddings,\n",
        "    get_chat_llm\n",
        ")\n",
        "\n",
        "# Initialize using service factories (supports OpenRouter + multi-provider)\n",
        "embeddings = get_default_embeddings()\n",
        "llm = get_chat_llm(temperature=0)\n",
        "\n",
        "print(f\" LLM initialized: {CHAT_MODEL}\")\n",
        "print(f\" Embeddings initialized: {EMBEDDING_MODEL}\")\n",
        "print(f\" Provider: {PROVIDER}\")\n",
        "\n",
        "# Connect to vector store\n",
        "if not VECTOR_DIR.exists():\n",
        "    raise FileNotFoundError(f\" Run 02_chunk_and_embed.ipynb first\")\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=str(VECTOR_DIR),\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=\"nawaloka\"\n",
        ")\n",
        "\n",
        "# Create retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": TOP_K_RESULTS}\n",
        ")\n",
        "\n",
        "print(f\" Connected to vector store\")\n",
        "print(f\" Collection size: {vectorstore._collection.count()} vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 1️ Standard RAG with Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Initialize RAG Service\n",
        "rag_service = RAGService(\n",
        "    retriever=retriever,\n",
        "    llm=llm,\n",
        "    k=TOP_K_RESULTS\n",
        ")\n",
        "\n",
        "print(\" RAGService initialized\")\n",
        "print(f\" Retrieval: top-{TOP_K_RESULTS} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Generate Answer with RAG Service\n",
        "USER_QUERY = \"Tell me about Nawaloka's cardiology services and health check packages.\"\n",
        "\n",
        "print(f\" Query: {USER_QUERY}\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"GENERATING ANSWER WITH RAG SERVICE...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "result = rag_service.generate(USER_QUERY)\n",
        "\n",
        "print(f\"\\n  Generation time: {result['generation_time']:.2f}s\")\n",
        "print(f\" Documents used: {result['num_docs']}\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ANSWER\")\n",
        "print(\"=\" * 80)\n",
        "print(result['answer'])\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVIDENCE URLS\")\n",
        "print(\"=\" * 80)\n",
        "for url in result['evidence_urls']:\n",
        "    print(f\"  - {url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# 2️ Cache-Augmented Generation (CAG) with Semantic Matching\n",
        "\n",
        "CAG uses lightweight semantic similarity to cache responses:\n",
        "- **FAQs**: Static questions, never expire\n",
        "- **History**: User queries, 24-hour TTL\n",
        "- **Matching**: Cosine similarity (catches paraphrases!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Initialize CAG Service with Semantic Cache\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create semantic cache (embedder required for similarity matching)\n",
        "cache = CAGCache(\n",
        "    cache_dir=CACHE_DIR,\n",
        "    embedder=embeddings,  # Uses same embeddings as vector store\n",
        "    similarity_threshold=0.90,  # Catches paraphrased questions\n",
        "    history_ttl_hours=24  # History expires after 24 hours\n",
        ")\n",
        "\n",
        "# Create CAG service\n",
        "cag_service = CAGService(\n",
        "    rag_service=rag_service,\n",
        "    cache=cache\n",
        ")\n",
        "\n",
        "print(\" CAGService initialized (semantic matching)\")\n",
        "print(f\" Cache directory: {CACHE_DIR}\")\n",
        "stats = cache.stats()\n",
        "print(f\" Cached responses: {stats['total_cached']}\")\n",
        "print(f\" Similarity threshold: {stats['similarity_threshold']}\")\n",
        "print(f\" History TTL: {stats['history_ttl_hours']}h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Load Known FAQs\n",
        "\n",
        "FAQs are defined in `config/faqs.yaml` and can be:\n",
        "1. **Loaded** into cache (registers questions)\n",
        "2. **Warmed** (generates responses via RAG)\n",
        "\n",
        "Once warmed, FAQs provide **instant responses** for common questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Load Known FAQs from Config (Optional)\n",
        "# FAQs are defined in config/faqs.yaml\n",
        "\n",
        "from context_engineering.config import KNOWN_FAQS\n",
        "\n",
        "print(f\" Found {len(KNOWN_FAQS)} FAQs in config/faqs.yaml\")\n",
        "print(\"\\n Sample FAQs:\")\n",
        "for faq in KNOWN_FAQS[:5]:\n",
        "    print(f\"   - {faq}\")\n",
        "print(f\"   ... and {len(KNOWN_FAQS) - 5} more\\n\")\n",
        "\n",
        "# Load FAQs into cache (this just registers them, doesn't generate responses yet)\n",
        "loaded = cag_service.load_faqs(KNOWN_FAQS)\n",
        "print(f\" Loaded {loaded} new FAQs into cache\")\n",
        "\n",
        "# To warm FAQs (generate responses), uncomment:\n",
        "# print(\"\\n Warming FAQs (this may take a few minutes)...\")\n",
        "# cag_service.warm_faqs()\n",
        "\n",
        "print(\"\\n Tip: Run cag_service.warm_faqs() to pre-generate FAQ responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Initialize CRAG Service\n",
        "from context_engineering.config import CRAG_EXPANDED_K\n",
        "\n",
        "crag_service = CRAGService(\n",
        "    retriever=retriever,\n",
        "    llm=llm,\n",
        "    initial_k=TOP_K_RESULTS,\n",
        "    expanded_k=CRAG_EXPANDED_K\n",
        ")\n",
        "\n",
        "print(\" CRAGService initialized\")\n",
        "print(f\" Initial retrieval: top-{TOP_K_RESULTS}\")\n",
        "print(f\" Corrective retrieval: top-{CRAG_EXPANDED_K}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Interactive Inference: Ask Your Own Question\n",
        "\n",
        " **IMPORTANT**: Run all cells above first to initialize all services!\n",
        "\n",
        "Run the cell below to ask your own question and get answers from **all 3 RAG systems** side-by-side!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Interactive Inference - Ask Your Own Question\n",
        "\n",
        "# INITIALIZE MISSING SERVICES (fallback if cells weren't run)\n",
        "if 'rag_service' not in dir():\n",
        "    rag_service = RAGService(retriever=retriever, llm=llm, k=TOP_K_RESULTS)\n",
        "if 'cag_service' not in dir():\n",
        "    cache = CAGCache(cache_dir=CACHE_DIR, embedder=embeddings, similarity_threshold=0.90)\n",
        "    cag_service = CAGService(rag_service=rag_service, cache=cache)\n",
        "if 'crag_service' not in dir():\n",
        "    from context_engineering.config import CRAG_EXPANDED_K\n",
        "    crag_service = CRAGService(retriever=retriever, llm=llm, initial_k=TOP_K_RESULTS, expanded_k=CRAG_EXPANDED_K)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" INTERACTIVE RAG INFERENCE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n Ask your question about Nawaloka Hospital...\\n\")\n",
        "\n",
        "# Input your question here\n",
        "YOUR_QUESTION = input(\" Your question: \")\n",
        "\n",
        "print(f\"\\n Processing query: '{YOUR_QUESTION}'\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run through all 3 RAG systems\n",
        "results = {}\n",
        "\n",
        "# 1. Standard RAG\n",
        "print(\"\\n1  Standard RAG\")\n",
        "print(\"-\" * 80)\n",
        "start = time.time()\n",
        "rag_result = rag_service.generate(YOUR_QUESTION)\n",
        "results['RAG'] = {\n",
        "    'answer': rag_result.get('answer', 'N/A'),\n",
        "    'time': rag_result.get('generation_time', rag_result.get('time', 0)),\n",
        "    'docs': rag_result.get('num_docs', len(rag_result.get('evidence_urls', []))),\n",
        "    'urls': rag_result.get('evidence_urls', [])\n",
        "}\n",
        "print(f\" Completed in {results['RAG']['time']:.2f}s\")\n",
        "print(f\" Documents retrieved: {results['RAG']['docs']}\")\n",
        "print(f\"\\n Answer:\")\n",
        "print(results['RAG']['answer'])\n",
        "print(f\"\\n Evidence URLs:\")\n",
        "for url in results['RAG']['urls'][:3]:\n",
        "    print(f\"   • {url}\")\n",
        "\n",
        "# 2. Cache-Augmented Generation\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\n2  Cache-Augmented Generation (CAG)\")\n",
        "print(\"-\" * 80)\n",
        "cag_result = cag_service.generate(YOUR_QUESTION, use_cache=True, verbose=False)\n",
        "results['CAG'] = {\n",
        "    'answer': cag_result.get('answer', 'N/A'),\n",
        "    'time': cag_result.get('generation_time', cag_result.get('time', 0)),\n",
        "    'docs': cag_result.get('num_docs', cag_result.get('docs_used', len(cag_result.get('evidence_urls', [])))),\n",
        "    'cache_hit': cag_result.get('cache_hit', False),\n",
        "    'urls': cag_result.get('evidence_urls', [])\n",
        "}\n",
        "print(f\" Completed in {results['CAG']['time']:.2f}s\")\n",
        "print(f\" Cache hit: {results['CAG']['cache_hit']}\")\n",
        "print(f\" Documents retrieved: {results['CAG']['docs']}\")\n",
        "print(f\"\\n Answer:\")\n",
        "print(results['CAG']['answer'])\n",
        "print(f\"\\n Evidence URLs:\")\n",
        "for url in results['CAG']['urls'][:3]:\n",
        "    print(f\"   • {url}\")\n",
        "\n",
        "# 3. Corrective RAG\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\n3  Corrective RAG (CRAG)\")\n",
        "print(\"-\" * 80)\n",
        "crag_result = crag_service.generate(YOUR_QUESTION, confidence_threshold=0.6, verbose=False)\n",
        "results['CRAG'] = {\n",
        "    'answer': crag_result.get('answer', 'N/A'),\n",
        "    'time': crag_result.get('generation_time', crag_result.get('time', 0)),\n",
        "    'docs': crag_result.get('docs_used', crag_result.get('num_docs', len(crag_result.get('evidence_urls', [])))),\n",
        "    'confidence': crag_result.get('confidence_final', crag_result.get('confidence', 0.0)),\n",
        "    'corrected': crag_result.get('correction_applied', False),\n",
        "    'urls': crag_result.get('evidence_urls', [])\n",
        "}\n",
        "print(f\" Completed in {results['CRAG']['time']:.2f}s\")\n",
        "print(f\" Confidence: {results['CRAG']['confidence']:.2f}\")\n",
        "print(f\" Correction applied: {results['CRAG']['corrected']}\")\n",
        "print(f\" Documents used: {results['CRAG']['docs']}\")\n",
        "print(f\"\\n Answer:\")\n",
        "print(results['CRAG']['answer'])\n",
        "print(f\"\\n Evidence URLs:\")\n",
        "for url in results['CRAG']['urls'][:3]:\n",
        "    print(f\"   • {url}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\n{'System':<15} {'Time (s)':<12} {'Docs':<8} {'Special Feature':<30}\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Standard RAG':<15} {results['RAG']['time']:<12.2f} {results['RAG']['docs']:<8} {'Baseline':<30}\")\n",
        "\n",
        "# CAG feature\n",
        "cag_feature = 'Cache: ' + ('HIT ⚡' if results['CAG']['cache_hit'] else 'MISS')\n",
        "print(f\"{'CAG':<15} {results['CAG']['time']:<12.2f} {results['CAG']['docs']:<8} {cag_feature:<30}\")\n",
        "\n",
        "# CRAG feature\n",
        "crag_conf = results['CRAG']['confidence']\n",
        "crag_emoji = ' ✅' if crag_conf > 0.7 else ' ⚠️'\n",
        "crag_feature = f\"Confidence: {crag_conf:.2f}{crag_emoji}\"\n",
        "print(f\"{'CRAG':<15} {results['CRAG']['time']:<12.2f} {results['CRAG']['docs']:<8} {crag_feature:<30}\")\n",
        "\n",
        "# Summary recommendation\n",
        "print(\"=\" * 80)\n",
        "print(\" RECOMMENDATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fastest = min(results.items(), key=lambda x: x[1]['time'])\n",
        "print(f\" Fastest: {fastest[0]} ({fastest[1]['time']:.2f}s)\")\n",
        "\n",
        "if results['CAG']['cache_hit']:\n",
        "    print(\" Best Choice: CAG (cache hit = instant response)\")\n",
        "elif results['CRAG']['confidence'] > 0.7:\n",
        "    print(\" Best Choice: CRAG (high confidence + corrective capability)\")\n",
        "else:\n",
        "    print(\" Best Choice: Standard RAG (reliable baseline)\")\n",
        "\n",
        "print(\"\\n Inference complete!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Test CAG Performance\n",
        "print(\"=\" * 80)\n",
        "print(\"CAG PERFORMANCE TEST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_queries = [\n",
        "    \"What are the visiting hours at Nawaloka?\",\n",
        "    \"How do I contact Nawaloka Hospital?\",\n",
        "    \"What services does Nawaloka provide?\"\n",
        "]\n",
        "\n",
        "# First run: populate cache\n",
        "print(\"\\n1  FIRST RUN (Populating cache)...\\n\")\n",
        "for query in test_queries:\n",
        "    result = cag_service.generate(query, use_cache=True, verbose=False)\n",
        "    print(f\"   Query: {query[:50]}...\")\n",
        "    print(f\"   Time: {result['generation_time']:.2f}s | Cache: {result['cache_hit']}\")\n",
        "    print()\n",
        "\n",
        "# Second run: cache hits\n",
        "print(\"\\n2  SECOND RUN (Using cache)...\\n\")\n",
        "for query in test_queries:\n",
        "    result = cag_service.generate(query, use_cache=True, verbose=False)\n",
        "    print(f\"   Query: {query[:50]}...\")\n",
        "    print(f\"   Time: {result['generation_time']:.2f}s | Cache: {result['cache_hit']}\")\n",
        "    if result['cache_hit']:\n",
        "        print(f\"    INSTANT response from cache!\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n Cache Statistics:\")\n",
        "stats = cache.stats()\n",
        "print(f\"   Total cached: {stats['total_cached']}\")\n",
        "print(f\"   Cache size: {stats['cache_size_kb']:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Test CRAG with Different Query Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Test CRAG with Different Query Types\n",
        "print(\"=\" * 80)\n",
        "print(\"CORRECTIVE RAG (CRAG) TEST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_cases = [\n",
        "    {\n",
        "        'query': \"cardiology\",\n",
        "        'label': \"Vague query (should trigger correction)\"\n",
        "    },\n",
        "    {\n",
        "        'query': \"What are Nawaloka Hospital's cardiology services and facilities?\",\n",
        "        'label': \"Specific query (should be confident)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    print(f\"\\nTest {i}/{len(test_cases)}: {test['label']}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    result = crag_service.generate(test['query'], confidence_threshold=0.6)\n",
        "    \n",
        "    print(f\"\\n Result:\")\n",
        "    print(f\"   Initial confidence: {result['confidence_initial']:.2f}\")\n",
        "    print(f\"   Final confidence: {result['confidence_final']:.2f}\")\n",
        "    print(f\"   Correction applied: {result['correction_applied']}\")\n",
        "    print(f\"   Documents used: {result['docs_used']}\")\n",
        "    print(f\"   Generation time: {result['generation_time']:.2f}s\")\n",
        "    print(\"\\n\" + \"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Check All Services Ready\n",
        "print(\" Checking if all services are initialized...\\n\")\n",
        "\n",
        "services_ready = True\n",
        "\n",
        "# Check RAGService\n",
        "try:\n",
        "    rag_service\n",
        "    print(\" RAGService: Ready\")\n",
        "except NameError:\n",
        "    print(\" RAGService: NOT initialized\")\n",
        "    services_ready = False\n",
        "\n",
        "# Check CAGService\n",
        "try:\n",
        "    cag_service\n",
        "    print(\" CAGService: Ready\")\n",
        "except NameError:\n",
        "    print(\" CAGService: NOT initialized\")\n",
        "    services_ready = False\n",
        "\n",
        "# Check CRAGService\n",
        "try:\n",
        "    crag_service\n",
        "    print(\" CRAGService: Ready\")\n",
        "except NameError:\n",
        "    print(\" CRAGService: NOT initialized\")\n",
        "    services_ready = False\n",
        "\n",
        "# Check Vector Store\n",
        "try:\n",
        "    vectorstore\n",
        "    print(\" Vector Store: Connected\")\n",
        "except NameError:\n",
        "    print(\" Vector Store: NOT connected\")\n",
        "    services_ready = False\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "if services_ready:\n",
        "    print(\" All services ready! You can run the remaining cells.\")\n",
        "else:\n",
        "    print(\"  Some services are missing. Please run all cells above first.\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Comprehensive Comparison: RAG vs CAG vs CRAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  Comprehensive Comparison\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" RAG vs CAG vs CRAG COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# comparison_query = \"What cardiology services are available at Nawaloka?\"\n",
        "comparison_query = \"What cardiology services are available at Nawaloka?\"\n",
        "\n",
        "# Test 1: Standard RAG\n",
        "print(f\"\\n1  Standard RAG...\")\n",
        "rag_result = rag_service.generate(comparison_query)\n",
        "print(f\"   ⏱  Time: {rag_result['generation_time']:.2f}s\")\n",
        "\n",
        "# Test 2: CAG (should use cache on second run)\n",
        "print(f\"\\n2  Cache-Augmented Generation (CAG)...\")\n",
        "cag_result = cag_service.generate(comparison_query, use_cache=True, verbose=False)\n",
        "print(f\"     Time: {cag_result['generation_time']:.2f}s\")\n",
        "print(f\"    Cache hit: {cag_result['cache_hit']}\")\n",
        "\n",
        "# Test 3: CRAG\n",
        "print(f\"\\n3  Corrective RAG (CRAG)...\")\n",
        "crag_result = crag_service.generate(comparison_query, confidence_threshold=0.6, verbose=False)\n",
        "print(f\"     Time: {crag_result['generation_time']:.2f}s\")\n",
        "print(f\"    Correction: {crag_result['correction_applied']}\")\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = [\n",
        "    {\n",
        "        'Technique': 'Standard RAG',\n",
        "        'Latency (s)': f\"{rag_result['generation_time']:.2f}\",\n",
        "        'Docs Retrieved': rag_result['num_docs'],\n",
        "        'Cache Used': 'No',\n",
        "        'Self-Correcting': 'No',\n",
        "        'Best For': 'General queries'\n",
        "    },\n",
        "    {\n",
        "        'Technique': 'CAG',\n",
        "        'Latency (s)': f\"{cag_result['generation_time']:.2f}\",\n",
        "        'Docs Retrieved': 'Cached' if cag_result['cache_hit'] else rag_result['num_docs'],\n",
        "        'Cache Used': 'Yes' if cag_result['cache_hit'] else 'No',\n",
        "        'Self-Correcting': 'No',\n",
        "        'Best For': 'Frequent queries'\n",
        "    },\n",
        "    {\n",
        "        'Technique': 'CRAG',\n",
        "        'Latency (s)': f\"{crag_result['generation_time']:.2f}\",\n",
        "        'Docs Retrieved': crag_result['docs_used'],\n",
        "        'Cache Used': 'No',\n",
        "        'Self-Correcting': 'Yes',\n",
        "        'Best For': 'Complex/uncertain queries'\n",
        "    }\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON TABLE\")\n",
        "print(\"=\" * 80)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY INSIGHTS\")\n",
        "print(\"=\" * 80)\n",
        "print(\" RAG: Baseline - reliable for general queries\")\n",
        "print(\" CAG: Fastest when cache hits - ideal for FAQs\")\n",
        "print(\" CRAG: Most accurate - self-corrects weak evidence\")\n",
        "print(\" HYBRID: Combine all three for production!\")\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "#  Summary\n",
        "\n",
        "All three RAG techniques implemented using service layer:\n",
        "-  **RAGService**: Standard RAG with modern LCEL\n",
        "-  **CAGService**: Cache-augmented generation with semantic matching\n",
        "-  **CRAGService**: Corrective RAG with confidence scoring\n",
        "\n",
        "**CAG Semantic Caching**:\n",
        "-  **Catches paraphrases**: \"What are visiting hours?\" matches \"Tell me the visiting hours\"\n",
        "-  **Two-tier**: Static FAQs (never expire) + Dynamic History (24h TTL)\n",
        "-  **Lightweight**: Only new queries need embedding, cached ones use stored embeddings\n",
        "-  **Fast lookup**: Cosine similarity is just a dot product (~1ms for 1000 entries)\n",
        "\n",
        "**Benefits of service-based architecture**:\n",
        "-  Reusable across notebooks and production code\n",
        "-  Easily testable\n",
        "-  Well-documented\n",
        "-  Maintainable (logic in one place)\n",
        "\n",
        "**OpenRouter Multi-Provider Support**:\n",
        "-  One API key → access to OpenAI, Anthropic, Google, Meta, DeepSeek\n",
        "-  Configure models in `config/models.yaml`\n",
        "-  Switch providers without code changes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
